{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models\n",
    "from torch.utils import model_zoo\n",
    "import pandas as pd\n",
    "import probabilities_to_decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "Using the ResNet50 architecture.\n",
      "<class 'torch.nn.parallel.data_parallel.DataParallel'>\n",
      "torch.Size([64, 3, 7, 7])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([])\n",
      "torch.Size([64, 64, 1, 1])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([])\n",
      "torch.Size([256, 64, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([])\n",
      "torch.Size([256, 64, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([])\n",
      "torch.Size([64, 256, 1, 1])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([])\n",
      "torch.Size([256, 64, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([])\n",
      "torch.Size([64, 256, 1, 1])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([])\n",
      "torch.Size([256, 64, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([])\n",
      "torch.Size([128, 256, 1, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([])\n",
      "torch.Size([512, 128, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([])\n",
      "torch.Size([512, 256, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([])\n",
      "torch.Size([128, 512, 1, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([])\n",
      "torch.Size([512, 128, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([])\n",
      "torch.Size([128, 512, 1, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([])\n",
      "torch.Size([512, 128, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([])\n",
      "torch.Size([128, 512, 1, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([])\n",
      "torch.Size([512, 128, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([])\n",
      "torch.Size([256, 512, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([])\n",
      "torch.Size([1024, 256, 1, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([])\n",
      "torch.Size([1024, 512, 1, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([])\n",
      "torch.Size([256, 1024, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([])\n",
      "torch.Size([1024, 256, 1, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([])\n",
      "torch.Size([256, 1024, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([])\n",
      "torch.Size([1024, 256, 1, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([])\n",
      "torch.Size([256, 1024, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([])\n",
      "torch.Size([1024, 256, 1, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([])\n",
      "torch.Size([256, 1024, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([])\n",
      "torch.Size([1024, 256, 1, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([])\n",
      "torch.Size([256, 1024, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([])\n",
      "torch.Size([1024, 256, 1, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([])\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([])\n",
      "torch.Size([2048, 512, 1, 1])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([])\n",
      "torch.Size([2048, 1024, 1, 1])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([])\n",
      "torch.Size([512, 2048, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([])\n",
      "torch.Size([2048, 512, 1, 1])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([])\n",
      "torch.Size([512, 2048, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([])\n",
      "torch.Size([2048, 512, 1, 1])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([])\n",
      "torch.Size([1000, 2048])\n",
      "torch.Size([1000])\n",
      "Model download completed.\n",
      "Model printing completed.\n"
     ]
    }
   ],
   "source": [
    "# load_pretrained_models.py\n",
    "print(torch.__version__)\n",
    "def load_model(model_name):\n",
    "\n",
    "    model_urls = {\n",
    "            'resnet50_trained_on_SIN': 'https://bitbucket.org/robert_geirhos/texture-vs-shape-pretrained-models/raw/6f41d2e86fc60566f78de64ecff35cc61eb6436f/resnet50_train_60_epochs-c8e5653e.pth.tar',\n",
    "            'resnet50_trained_on_SIN_and_IN': 'https://bitbucket.org/robert_geirhos/texture-vs-shape-pretrained-models/raw/60b770e128fffcbd8562a3ab3546c1a735432d03/resnet50_train_45_epochs_combined_IN_SF-2a0d100e.pth.tar',\n",
    "            'resnet50_trained_on_SIN_and_IN_then_finetuned_on_IN': 'https://bitbucket.org/robert_geirhos/texture-vs-shape-pretrained-models/raw/60b770e128fffcbd8562a3ab3546c1a735432d03/resnet50_finetune_60_epochs_lr_decay_after_30_start_resnet50_train_45_epochs_combined_IN_SF-ca06340c.pth.tar',\n",
    "            'alexnet_trained_on_SIN': 'https://bitbucket.org/robert_geirhos/texture-vs-shape-pretrained-models/raw/0008049cd10f74a944c6d5e90d4639927f8620ae/alexnet_train_60_epochs_lr0.001-b4aa5238.pth.tar',\n",
    "    }\n",
    "    if \"resnet50\" in model_name:\n",
    "        print(\"Using the ResNet50 architecture.\")\n",
    "        model = torchvision.models.resnet50(pretrained=False)\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "        checkpoint = model_zoo.load_url(model_urls[model_name])\n",
    "    elif \"vgg16\" in model_name:\n",
    "        print(\"Using the VGG-16 architecture.\")\n",
    "\n",
    "        # download model from URL manually and save to desired location\n",
    "        filepath = \"./vgg16_train_60_epochs_lr0.01-6c6fcc9f.pth.tar\"\n",
    "\n",
    "        assert os.path.exists(filepath), \"Please download the VGG model yourself from the following link and save it locally: https://drive.google.com/drive/folders/1A0vUWyU6fTuc-xWgwQQeBvzbwi6geYQK (too large to be downloaded automatically like the other models)\"\n",
    "\n",
    "        model = torchvision.models.vgg16(pretrained=False)\n",
    "        model.features = torch.nn.DataParallel(model.features)\n",
    "        model.cuda()\n",
    "        checkpoint = torch.load(filepath)\n",
    "\n",
    "\n",
    "    elif \"alexnet\" in model_name:\n",
    "        print(\"Using the AlexNet architecture.\")\n",
    "        model = torchvision.models.alexnet(pretrained=False)\n",
    "        model.features = torch.nn.DataParallel(model.features)\n",
    "        model.cuda()\n",
    "        checkpoint = model_zoo.load_url(model_urls[model_name])\n",
    "    else:\n",
    "        raise ValueError(\"unknown model architecture.\")\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "#def softmax(model):\n",
    "#    assert len(model.shape) == 1\n",
    "#    s = np.max(model, axis=1)\n",
    "#    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "#    e_x = np.exp(z - s)\n",
    "#    div = np.sum(e_x, axis=1)\n",
    "#    div = div[:, np.newaxis] # dito\n",
    "#    return e_x / div\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Abbreviations:\n",
    "    # SIN = Stylized-ImageNet\n",
    "    # IN = normal, standard ImageNet\n",
    "\n",
    "    model_A = \"resnet50_trained_on_SIN\"\n",
    "    model_B = \"resnet50_trained_on_SIN_and_IN\"\n",
    "    model_C = \"resnet50_trained_on_SIN_and_IN_then_finetuned_on_IN\"\n",
    "\n",
    "    # Note: these two models correspond to the ones reported in Figure 11.\n",
    "    # Hyperparameters (learning rate etc.) were NOT optimised (this was\n",
    "    # done in the rebuttal period with limited time), thus these\n",
    "    # models have lower performance than a typical model would have.\n",
    "    # If peak performance is important to you, I suggest to train the model\n",
    "    # yourself.\n",
    "    model_D = \"vgg16_trained_on_SIN\"\n",
    "    model_E = \"alexnet_trained_on_SIN\"\n",
    "\n",
    "    model = load_model(model_A) # change to different model as desired\n",
    "    \n",
    "    print(type(model))\n",
    "    #print(type(model.module.cpu().state_dict().values()))\n",
    "    #print(model.module.cpu().state_dict().items())\n",
    "    tensor_list = []\n",
    "    for k, v in model.module.cpu().state_dict().items():\n",
    "        tensor_list.append(v)\n",
    "        print(v.shape)\n",
    "    #tensors = torch.Tensor(tensor_list)\n",
    "    #cat_tensor = torch.cat(tensor_list, dim=0)\n",
    "    #print(v.shape)\n",
    "    #print(cat_tensor.shape)\n",
    "        #gg = torch.cat((v))\n",
    "    #jj = torch.nn.Softmax(model.module.state_dict().values())\n",
    "    #print(jj)\n",
    "    #lst = []\n",
    "    #lst = list(model.module.cpu().state_dict().values())\n",
    "    #for k, v in model.module.cpu().state_dict().items():\n",
    "    #    _\n",
    "    #print(v)\n",
    "    #m = torch.nn.Softmax(dim=1)\n",
    "    #softmax_output_numpy = m(mm)\n",
    "    #softmax_output_numpy = np.array(lst, dtype=object)\n",
    "    #print(len(softmax_output_numpy.shape),\"length\")\n",
    "    #print(\"Softmax\", softmax(softmax_output_numpy))\n",
    "    # create mlapping\n",
    "    #mapping = probabilities_to_decision.ImageNetProbabilitiesTo16ClassesMapping()\n",
    "    # obtain decision\n",
    "    #decision_from_16_classes = mapping.probabilities_to_decision(softmax_output_numpy)\n",
    "\n",
    "    print(\"Model download completed.\")\n",
    "\n",
    "    # sanity check: print state_dict\n",
    "    #try:\n",
    "    #    for k, v in model.module.state_dict().items():\n",
    "    #        print(v)\n",
    "    #except AttributeError:\n",
    "    #    for k, v in model.state_dict().items():\n",
    "    #        print(k)\n",
    "\n",
    "    print(\"Model printing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7127, -0.0707, -0.9960],\n",
      "        [ 0.6772,  1.9456, -0.5676]])\n",
      "tensor([[0.6105, 0.2789, 0.1106],\n",
      "        [0.2065, 0.7341, 0.0595]])\n"
     ]
    }
   ],
   "source": [
    "m = torch.nn.Softmax(dim=1)\n",
    "input = torch.randn(2, 3)\n",
    "output = m(input)\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
